{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yes_no_answer():\n",
    "    '''\n",
    "    \n",
    "    Gets user response as 'y' or 'n' or 'yes' or 'no' or their case variations.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    while True:\n",
    "        reply = str(input('Combine?: (y/n): ')).lower().strip()\n",
    "        \n",
    "        if reply == 'y' or reply == 'n':\n",
    "            break\n",
    "        else:\n",
    "            print(\"Please select 'yes' or 'no'\")\n",
    "    \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_dummy_scheds(df):\n",
    "    '''\n",
    "    \n",
    "    Dummy schedules are identified when their Effective From Date is the same as Effective To Date.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    idx = [i for i in range(df.shape[0]) if df['eff_from'][i] == df['eff_to'][i]]\n",
    "    df = df.drop(idx, axis = 0)\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(['index'], axis = 1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_noncurrent_scheds(df, tz):\n",
    "    '''\n",
    "    \n",
    "    Schedules that are, as of today, yet to commence or are no longer in operations.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    today = datetime.now(tz).date()\n",
    "    idx = [i for i in range(df.shape[0]) \n",
    "           if (today <=  datetime.date(df['eff_from'][i])) or (today >= datetime.date(df['eff_to'][i]))]\n",
    "    df = df.drop(idx, axis = 0)\n",
    "    df = df.reset_index()\n",
    "    df = df.drop('index', axis = 1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_frequency(df):\n",
    "    '''\n",
    "    \n",
    "    Standardize <frequency> values for all schedules as a list of numbers that can take values from 1 to 7 for the\n",
    "    weekdays on which a schedule is operational.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "# Convert frequency into a string of numbers\n",
    "    for i in range(df.shape[0]):\n",
    "        if isinstance(df.frequency[i], str):\n",
    "            df.frequency[i] = '1234567'\n",
    "        else:\n",
    "            df.frequency[i] = str(int(df.frequency[i]))\n",
    "\n",
    "# Convert string of numbers into list of numbers\n",
    "        df.frequency[i] = list(df.frequency[i]) # Split str\n",
    "        df.frequency[i] = list(map(int, df.frequency[i])) # Convert from str to int\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_pairs(df, human_intel = 'n'):\n",
    "    '''\n",
    "    \n",
    "    In raw data, each schedule has two legs of information: at origin and at destination. These two legs are\n",
    "    present as different records. Identify pairs and combine them into a single record.\n",
    "    \n",
    "    Allow for human intervetion to identify possible pairs when poor/missing data does not allow for automated \n",
    "    identification.\n",
    "    \n",
    "    Embedded helper function combine_pairs_func() appropriately combines records [i] and [j] and marks record [j] \n",
    "    for deletion.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def combine_pairs_func(i, j, drop_idx):\n",
    "\n",
    "        drop_idx.append(j)\n",
    "        \n",
    "        if df.to_time[i] == '': \n",
    "            df.to_time[i] = df.to_time[j]\n",
    "        else: \n",
    "            df.from_time[i] = df.from_time[j]\n",
    "            \n",
    "        return drop_idx\n",
    "    \n",
    "    drop_idx = []        \n",
    "    \n",
    "    # Compare a record with only subsequent records and as long as the subsequent record has not already been\n",
    "    # paired with some other record.\n",
    "    for i in range(df.shape[0] - 1):\n",
    "        if i not in drop_idx:\n",
    "            for j in range(i+1, df.shape[0]):\n",
    "                if j not in drop_idx:\n",
    "                    \n",
    "                    # Automated pairing\n",
    "                    if human_intel == 'n':\n",
    "                        if df.flight[i] == df.flight[j] and df.frequency[i] == df.frequency[j]:\n",
    "                            if df['from'][i] == df['from'][j] and df.to[i] == df.to[j]:\n",
    "                                drop_idx = combine_pairs_func(i, j, drop_idx)\n",
    "                                \n",
    "                    # Pairing with human intervention by relaxing the identical frequency constraint\n",
    "                    else:\n",
    "                        if df.flight[i] == df.flight[j]:\n",
    "                            if df.to[i] == df.to[j] and df['from'][i] == df['from'][j]:\n",
    "                                if df.to_time[i] == '' and df.from_time[j] == '' and df.from_time[i] != '' and df.to_time[j] != '':\n",
    "                                    display(df.iloc[i, :8], df.iloc[j, :8])\n",
    "                                    display('Possibly Pairs?')\n",
    "                                    reply = get_yes_no_answer()\n",
    "                                    \n",
    "                                    if reply == 'y':\n",
    "                                        drop_idx = combine_pairs_func(i, j, drop_idx)\n",
    "                                    \n",
    "                                elif df.to_time[i] != '' and df.from_time[j] != '' and df.from_time[i] == '' and df.to_time[j] == '':\n",
    "                                    display(df.iloc[i, :8], df.iloc[j, :8])\n",
    "                                    display('Possibly Pairs?')\n",
    "                                    reply = get_yes_no_answer()\n",
    "                                    \n",
    "                                    if reply == 'y':\n",
    "                                        drop_idx = combine_pairs_func(i, j, drop_idx)\n",
    "                            \n",
    "    \n",
    "    df = df.drop(drop_idx, axis = 0)\n",
    "    df = df.reset_index()\n",
    "    df = df.drop('index', axis = 1)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_freq(df, human_intel = 'n'):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Combine multiple schedule records where the schedules in the set differ only in their frequencies. This \n",
    "    requires no human intervention.\n",
    "\n",
    "    Combine multiple schedules where the schedules in the set differ not only in their frequencies but vary \n",
    "    slightly in their <from_time> (or <to_time>) values. This requires human intervention\n",
    "    \n",
    "    Embedded helper function combine_frq_func() appropriately combines records [i] and [j] and marks record [j] \n",
    "    for deletion.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def combine_freq_func(i, j, drop_idx):\n",
    "        drop_idx.append(j)\n",
    "        df.frequency[i] = df.frequency[i] + df.frequency[j]\n",
    "        df.frequency[i] = list(set(df.frequency[i]))\n",
    "        return drop_idx\n",
    "        \n",
    "    drop_idx = []    \n",
    "    \n",
    "    for i in range(df.shape[0] - 1):\n",
    "        if i not in drop_idx:\n",
    "            for j in range(i+1, df.shape[0]):\n",
    "                if j not in drop_idx:\n",
    "                    \n",
    "                    # Automated Merging\n",
    "                    if human_intel == 'n':\n",
    "                        if df.flight[i] == df.flight[j] and df['from'][i] == df['from'][j] and df.to[i] == df.to[j]:\n",
    "                            if df.to_time[i] == df.to_time[j] and df.from_time[i] == df.from_time[j]:\n",
    "                                drop_idx = combine_freq_func(i, j, drop_idx)\n",
    "                    \n",
    "                    # Merging with human intervention by requiring only either to_time or from_time be identical  \n",
    "                    else:\n",
    "                        if df.flight[i] == df.flight[j] and df['from'][i] == df['from'][j] and df.to[i] == df.to[j]:\n",
    "                            if df.to_time[i] == df.to_time[j] or df.from_time[i] == df.from_time[j]:\n",
    "                                \n",
    "                                display(df.iloc[i, :8], '\\n', df.iloc[j, :8])\n",
    "                                display('Probably the same flight.')\n",
    "\n",
    "                                reply = get_yes_no_answer()\n",
    "    \n",
    "                                if reply == 'y':\n",
    "                                    drop_idx = combine_freq_func(i, j, drop_idx)\n",
    "      \n",
    "    df = df.drop(drop_idx, axis = 0)\n",
    "    df = df.reset_index()\n",
    "    df = df.drop('index', axis = 1)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_orphan_scheds(dat):\n",
    "    \n",
    "    trash_idx = [i for i in range(dat.shape[0]) if dat.to_time[i] == '' or dat.from_time[i] == '']\n",
    "\n",
    "    dat = dat.drop(trash_idx, axis = 0)\n",
    "    dat = dat.reset_index()\n",
    "    dat = dat.drop('index', axis = 1)\n",
    "    dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing for Vistara\n",
      "(Stage I) Read raw data: has 554 records\n",
      "(Stage II) Deleted header rows: now has 515 records\n",
      "(Stage III) Updated the <From>(or <To>) column with Station name\n",
      "(Stage IV) Fixed known city name spelling errors\n",
      "(Stage V) Replaced city names with corresponding IATA codes and saved the processed file\n",
      "\n",
      "Processing for SpiceJet\n",
      "(Stage I) Read raw data: has 1680 records\n",
      "(Stage II) Deleted header rows: now has 1592 records\n",
      "(Stage III) Updated the <From>(or <To>) column with Station name\n",
      "(Stage IV) Fixed known city name spelling errors\n",
      "(Stage V) Replaced city names with corresponding IATA codes and saved the processed file\n",
      "\n",
      "Processing for TruJet\n",
      "(Stage I) Read raw data: has 201 records\n",
      "(Stage II) Deleted header rows: now has 174 records\n",
      "(Stage III) Updated the <From>(or <To>) column with Station name\n",
      "(Stage IV) Fixed known city name spelling errors\n",
      "(Stage V) Replaced city names with corresponding IATA codes and saved the processed file\n",
      "\n",
      "Processing for Air_India\n",
      "(Stage I) Read raw data: has 1204 records\n",
      "(Stage II) Deleted header rows: now has 1125 records\n",
      "(Stage III) Updated the <From>(or <To>) column with Station name\n",
      "(Stage IV) Fixed known city name spelling errors\n",
      "(Stage V) Replaced city names with corresponding IATA codes and saved the processed file\n",
      "\n",
      "Processing for Star_Air\n",
      "(Stage I) Read raw data: has 83 records\n",
      "(Stage II) Deleted header rows: now has 74 records\n",
      "(Stage III) Updated the <From>(or <To>) column with Station name\n",
      "(Stage IV) Fixed known city name spelling errors\n",
      "(Stage V) Replaced city names with corresponding IATA codes and saved the processed file\n",
      "\n",
      "Processing for Indigo\n",
      "(Stage I) Read raw data: has 4501 records\n",
      "(Stage II) Deleted header rows: now has 4354 records\n",
      "(Stage III) Updated the <From>(or <To>) column with Station name\n",
      "(Stage IV) Fixed known city name spelling errors\n",
      "(Stage V) Replaced city names with corresponding IATA codes and saved the processed file\n",
      "\n",
      "Processing for Air_Asia\n",
      "(Stage I) Read raw data: has 641 records\n",
      "(Stage II) Deleted header rows: now has 608 records\n",
      "(Stage III) Updated the <From>(or <To>) column with Station name\n",
      "(Stage IV) Fixed known city name spelling errors\n",
      "(Stage V) Replaced city names with corresponding IATA codes and saved the processed file\n",
      "\n",
      "Processing for Go_Air\n",
      "(Stage I) Read raw data: has 1051 records\n",
      "(Stage II) Deleted header rows: now has 1003 records\n",
      "(Stage III) Updated the <From>(or <To>) column with Station name\n",
      "(Stage IV) Fixed known city name spelling errors\n",
      "(Stage V) Replaced city names with corresponding IATA codes and saved the processed file\n",
      "\n",
      "Processing for Alliance_Air\n",
      "(Stage I) Read raw data: has 498 records\n",
      "(Stage II) Deleted header rows: now has 431 records\n",
      "(Stage III) Updated the <From>(or <To>) column with Station name\n",
      "(Stage IV) Fixed known city name spelling errors\n",
      "(Stage V) Replaced city names with corresponding IATA codes and saved the processed file\n"
     ]
    }
   ],
   "source": [
    "raw_data_dir_path ='./data/raw/excel'\n",
    "stage_1_data_dir_path = './data/processed/stage-1'\n",
    "\n",
    "# Getting list of files in the directory\n",
    "_, _, raw_file_names = next(os.walk(raw_data_dir_path))\n",
    "_, _, stage_1_file_names = next(os.walk(stage_1_data_dir_path))\n",
    "\n",
    "# Retaining only Excel files from raw and stage-1 data\n",
    "raw_file_names = [i for i in raw_file_names if '.xlsx' in i]\n",
    "stage_1_file_names = [i for i in stage_1_file_names if '.xlsx' in i]\n",
    "\n",
    "# Loading the required dictionaries\n",
    "##### Correcting know city name spelling errors\n",
    "with open('./data/processed/dicts/city_spelling_corrected_dict.txt', 'rb') as handle:\n",
    "    city_names = pickle.loads(handle.read())\n",
    "##### Mapping city names to corresponding IATA codes\n",
    "with open('./data/processed/dicts/city_to_codes_dict.txt', 'rb') as handle:\n",
    "    city_to_codes = pickle.loads(handle.read()) \n",
    "\n",
    "file_names = [value for value in raw_file_names if value not in stage_1_file_names]\n",
    "\n",
    "# Processing individual files\n",
    "for file_name in file_names:\n",
    "    print('\\nProcessing for %s' % file_name.split('.')[0])\n",
    "    \n",
    "    readpath = raw_data_dir_path + '/' + file_name\n",
    "    dat = read_excel_data(readpath)\n",
    "    print('(Stage I) Read raw data: has %i records' % dat.shape[0])\n",
    "    \n",
    "    dat = wrangle_rows(dat)\n",
    "    print('(Stage II) Deleted header rows: now has %i records' % dat.shape[0])\n",
    "    \n",
    "    dat = wrangle_cols(dat)\n",
    "    print('(Stage III) Updated the <From>(or <To>) column with Station name')\n",
    "    \n",
    "    dat = fix_city_spelling(dat, city_names)\n",
    "    print('(Stage IV) Fixed known city name spelling errors')\n",
    "    \n",
    "    flag, missing_city_names, dat = wrangle_iata_codes(dat, city_to_codes)\n",
    "    if flag == 'up':\n",
    "        savepath = './data/processed/stage-1/' + file_name\n",
    "        dat.to_excel(savepath)\n",
    "        print('(Stage V) Replaced city names with corresponding IATA codes and saved the processed file')\n",
    "    else:\n",
    "        print('(Stage V)There are missing IATA codes and/or incorrectly spelt city names.')\n",
    "        print('File partially processed and not saved.')\n",
    "        print('The list of city names either with wrong spelling or missing IATA codes\\n', missing_city_names)\n",
    "        print('Fix errors in the data dictionaries and process again.')\n",
    "        print(' Process will begin from the file that had missing IATA codes')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
